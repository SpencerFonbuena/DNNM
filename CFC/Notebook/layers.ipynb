{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### [#] => There is a written explanation\n",
    "> #### (#) => There is a code explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import math\n",
    "\n",
    "class DSW_embedding(nn.Module):\n",
    "    def __init__(self, seg_len, d_model):\n",
    "        super(DSW_embedding, self).__init__()\n",
    "        self.seg_len = seg_len\n",
    "\n",
    "        self.linear = nn.Linear(seg_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, ts_len, ts_dim = x.shape\n",
    "        print(x.shape)\n",
    "        x_segment = rearrange(x, 'b (seg_num seg_len) d -> (b d seg_num) seg_len', seg_len = self.seg_len)\n",
    "        print(x_segment.shape)\n",
    "        x_embed = self.linear(x_segment)\n",
    "        print(x_embed.shape)\n",
    "        x_embed = rearrange(x_embed, '(b d seg_num) d_model -> b d seg_num d_model', b = batch, d = ts_dim)\n",
    "        print(x_embed.shape)\n",
    "        \n",
    "        #return x_embed\n",
    "embed = DSW_embedding(20,512)\n",
    "data = torch.randn(16,100,20)\n",
    "embed(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "> ### There is nothing special about these attention layers. They are the normal implementation. All of the novelty of the cross-former comes in how they are used, and the embedding and reshaping, but this is halal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "class FullAttention(nn.Module):\n",
    "    '''\n",
    "    The Attention operation\n",
    "    '''\n",
    "    def __init__(self, scale=None, attention_dropout=0.1):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1./sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "        \n",
    "        return V.contiguous()\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    '''\n",
    "    The Multi-head Self-Attention (MSA) Layer\n",
    "    '''\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_values=None, mix=True, dropout = 0.1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = FullAttention(scale=None, attention_dropout = dropout)\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        self.mix = mix\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "        )\n",
    "        if self.mix:\n",
    "            out = out.transpose(2,1).contiguous()\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Stage Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ### This two stage attention is just an explicit strung out version. You wouldn't want to use this version in production\n",
    ">> #### I am going to change all of this for the production code, but for knowledge sake, I'll label everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageAttentionLayer(nn.Module):\n",
    "    '''\n",
    "    The Two Stage Attention (TSA) Layer\n",
    "    input/output shape: [batch_size, Data_dim(D), Seg_num(L), d_model]\n",
    "    '''\n",
    "    def __init__(self, seg_num, factor, d_model, n_heads, d_ff = None, dropout=0.1):\n",
    "        super(TwoStageAttentionLayer, self).__init__()\n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.time_attention = AttentionLayer(d_model, n_heads, dropout = dropout)\n",
    "        self.dim_sender = AttentionLayer(d_model, n_heads, dropout = dropout)\n",
    "        self.dim_receiver = AttentionLayer(d_model, n_heads, dropout = dropout)\n",
    "        self.router = nn.Parameter(torch.randn(seg_num, factor, d_model))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.norm4 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(d_ff, d_model))\n",
    "        self.MLP2 = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Cross Time Stage: Directly apply MSA to each dimension\n",
    "        batch = x.shape[0]\n",
    "            # [Be sure that seg_num is in the 1 dimension, so that is what the MHA will attend to]\n",
    "        time_in = rearrange(x, 'b ts_d seg_num d_model -> (b ts_d) seg_num d_model')\n",
    "            \n",
    "            # [MHA]\n",
    "        time_enc = self.time_attention(\n",
    "            time_in, time_in, time_in\n",
    "        )\n",
    "            # [Add and Norm]\n",
    "        dim_in = time_in + self.dropout(time_enc)\n",
    "        dim_in = self.norm1(dim_in)\n",
    "\n",
    "            # [Feed Forward Add and Norm]\n",
    "        dim_in = dim_in + self.dropout(self.MLP1(dim_in))\n",
    "        dim_in = self.norm2(dim_in)\n",
    "\n",
    "\n",
    "        #Cross Dimension Stage: use a small set of learnable vectors to aggregate and distribute messages to build the D-to-D connection\n",
    "           \n",
    "            # [Rearrange so that ts_d is in the 1 dimension, so that is what the MHA will attend to]\n",
    "        dim_send = rearrange(dim_in, '(b ts_d) seg_num d_model -> (b seg_num) ts_d d_model', b = batch)\n",
    "            # [The router is a proxy for the queries. Instead of ]\n",
    "        batch_router = repeat(self.router, 'seg_num factor d_model -> (repeat seg_num) factor d_model', repeat = batch)\n",
    "        dim_buffer = self.dim_sender(batch_router, dim_send, dim_send)\n",
    "        dim_receive = self.dim_receiver(dim_send, dim_buffer, dim_buffer)\n",
    "\n",
    "            # [Add and Norm]\n",
    "        dim_enc = dim_send + self.dropout(dim_receive)\n",
    "        dim_enc = self.norm3(dim_enc)\n",
    "\n",
    "            # [Feed Forward Add and Norm]\n",
    "        dim_enc = dim_enc + self.dropout(self.MLP2(dim_enc))\n",
    "        dim_enc = self.norm4(dim_enc)\n",
    "\n",
    "        final_out = rearrange(dim_enc, '(b seg_num) ts_d d_model -> b ts_d seg_num d_model', b = batch)\n",
    "\n",
    "        return final_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how the router works to diminish the size and complexity of cross dimension encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_router = torch.randn(16,10,512)\n",
    "keys = torch.randn(16,120,512).transpose(-1,-2)\n",
    "values = torch.randn(16,120,512)\n",
    "\n",
    "attention = torch.matmul(queries_router, keys)\n",
    "attention.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Encoder consists of three blocks, the segment merging, the scale block, and the encoder block. As opposed to a normal transformer, where each encoder block remains hidden, Crossformer encoder blocks encode information at different scales, adn thust he output of each hidden layer needs to be preserved. In the Scale Block, there is a parameter that decides how many TSA blocks you want. This means that each encoder block could consist of 1 merging layer, and then 5 tsa blocks. In the encoder module, you then use an iterator to go through each block, and perform attention at varying granularities of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Merging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### [1]: This is just making sure there are the correct number of segment lengths to combine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "#from modules.layers import FullAttention, AttentionLayer, TwoStageAttentionLayer\n",
    "from math import ceil\n",
    "\n",
    "class SegMerging(nn.Module):\n",
    "    '''\n",
    "    Segment Merging Layer.\n",
    "    The adjacent `win_size' segments in each dimension will be merged into one segment to\n",
    "    get representation of a coarser scale\n",
    "    we set win_size = 2 in our paper\n",
    "    '''\n",
    "    def __init__(self, d_model, win_size, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.win_size = win_size\n",
    "        self.linear_trans = nn.Linear(win_size * d_model, d_model)\n",
    "        self.norm = norm_layer(win_size * d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, ts_d, L, d_model\n",
    "        \"\"\"\n",
    "        # [1]\n",
    "        batch_size, ts_d, seg_num, d_model = x.shape\n",
    "        pad_num = seg_num % self.win_size\n",
    "        if pad_num != 0: \n",
    "            pad_num = self.win_size - pad_num\n",
    "            # (1)\n",
    "            x = torch.cat((x, x[:, :, -pad_num:, :]), dim = -2)\n",
    "\n",
    "        # (2)\n",
    "        seg_to_merge = []\n",
    "        for i in range(self.win_size):\n",
    "            seg_to_merge.append(x[:, :, i::self.win_size, :])\n",
    "        x = torch.cat(seg_to_merge, -1)  # [B, ts_d, seg_num/win_size, win_size*d_model]\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.linear_trans(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) => The [-1:] in the 2nd dimension is just saying, take the last value, and concatenate it on there. This way, our padding just provides a couple duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(16).reshape(2,2,2,2)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "x = torch.cat((x, x[:, :, -1:, :]), dim = -2)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) => This does the exact same thing as x.reshape(1,2,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15],\n",
      "          [16, 17, 18, 19]],\n",
      "\n",
      "         [[20, 21, 22, 23],\n",
      "          [24, 25, 26, 27],\n",
      "          [28, 29, 30, 31],\n",
      "          [32, 33, 34, 35],\n",
      "          [36, 37, 38, 39]]]])\n",
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15],\n",
      "          [16, 17, 18, 19]],\n",
      "\n",
      "         [[20, 21, 22, 23],\n",
      "          [24, 25, 26, 27],\n",
      "          [28, 29, 30, 31],\n",
      "          [32, 33, 34, 35],\n",
      "          [36, 37, 38, 39]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seg_to_merge = []\n",
    "win_size = 2\n",
    "x = torch.arange(40).reshape(1,2,10,2)\n",
    "o,t,th,f = x.shape\n",
    "for i in range(win_size):\n",
    "    seg_to_merge.append(x[:, :, i::win_size, :])\n",
    "x = torch.cat(seg_to_merge, -1)\n",
    "print(x)\n",
    "x = x.reshape(o,t,int(th/win_size), f*win_size)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [1,2,3,4,5]\n",
    "torch.tensor(mylist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Block "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### [General]: Notice the overall structure. First, there is a segment layer, then there is 'depth' number of TSA layers\n",
    "> #### [1]: this is there to run through the TSA layer before any merging occurs on the initial input.| Encoder module [2]\n",
    "> #### [2]: Start out as an empty module list, but fills up with however many TSA layers we want\n",
    "> #### [3]: This line does the filling of the encode layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scale_block(nn.Module):\n",
    "    '''\n",
    "    We can use one segment merging layer followed by multiple TSA layers in each scale\n",
    "    the parameter `depth' determines the number of TSA layers used in each scale\n",
    "    We set depth = 1 in the paper\n",
    "\n",
    "    Parameters:\n",
    "    win_size: How big of segments we want to combine. If it is 2, then we will be twice as coarse each go around\n",
    "    d_model: # of features\n",
    "    n_heads: # of heads\n",
    "    d_ff: Feed forward dimension\n",
    "    depth: # of TSA layers in between each merge\n",
    "\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, win_size, d_model, n_heads, d_ff, depth, dropout, \\\n",
    "                    seg_num = 10, factor=10):\n",
    "        super(scale_block, self).__init__()\n",
    "        # [1]\n",
    "        if (win_size > 1):\n",
    "            self.merge_layer = SegMerging(d_model, win_size, nn.LayerNorm)\n",
    "        else:\n",
    "            self.merge_layer = None\n",
    "        # [2]\n",
    "        self.encode_layers = nn.ModuleList()\n",
    "\n",
    "        # [3]\n",
    "        for i in range(depth):\n",
    "            self.encode_layers.append(TwoStageAttentionLayer(seg_num, factor, d_model, n_heads, \\\n",
    "                                                        d_ff, dropout))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, ts_dim, _, _ = x.shape\n",
    "\n",
    "        if self.merge_layer is not None:\n",
    "            x = self.merge_layer(x)\n",
    "        \n",
    "        for layer in self.encode_layers:\n",
    "            x = layer(x)        \n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### [1]: This starts out as an empty modulelist, but fills up with however many scale_blocks we want\n",
    "> #### [2]: Notice that the win_size is 1. I believe this is here so that we don't start out segmenting, as is written in the paper. After we process the initial input, we can then go through the TSA, and segment to get coarser representations as we go along. | Scale Block [1]\n",
    "> #### [3]: From here on out, I think it's just business as usual. Stack up the encoder blocks like you would in any other transformer based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    The Encoder of Crossformer.\n",
    "    '''\n",
    "    def __init__(self, e_blocks, win_size, d_model, n_heads, d_ff, block_depth, dropout,\n",
    "                in_seg_num = 10, factor=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        # [1]\n",
    "        self.encode_blocks = nn.ModuleList()\n",
    "        # [2]\n",
    "        self.encode_blocks.append(scale_block(1, d_model, n_heads, d_ff, block_depth, dropout,\\\n",
    "                                            in_seg_num, factor))\n",
    "        # [3]\n",
    "        for i in range(1, e_blocks):\n",
    "            self.encode_blocks.append(scale_block(win_size, d_model, n_heads, d_ff, block_depth, dropout,\\\n",
    "                                            ceil(in_seg_num/win_size**i), factor))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode_x = []\n",
    "        encode_x.append(x)\n",
    "        \n",
    "        for block in self.encode_blocks:\n",
    "            x = block(x)\n",
    "            encode_x.append(x)\n",
    "\n",
    "        return encode_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    The decoder layer of Crossformer, each layer will make a prediction at its scale\n",
    "    '''\n",
    "    def __init__(self, seg_len, d_model, n_heads, d_ff=None, dropout=0.1, out_seg_num = 10, factor = 10):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = TwoStageAttentionLayer(out_seg_num, factor, d_model, n_heads, \\\n",
    "                                d_ff, dropout)    \n",
    "        self.cross_attention = AttentionLayer(d_model, n_heads, dropout = dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_model),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(d_model, d_model))\n",
    "        self.linear_pred = nn.Linear(d_model, seg_len)\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        '''\n",
    "        x: the output of last decoder layer\n",
    "        cross: the output of the corresponding encoder layer\n",
    "        '''\n",
    "\n",
    "        batch = x.shape[0]\n",
    "\n",
    "        # [Self MHA] | Why is there no mask?\n",
    "        x = self.self_attention(x)\n",
    "        x = rearrange(x, 'b ts_d out_seg_num d_model -> (b ts_d) out_seg_num d_model')\n",
    "        \n",
    "        cross = rearrange(cross, 'b ts_d in_seg_num d_model -> (b ts_d) in_seg_num d_model')\n",
    "        \n",
    "        # [MHA]\n",
    "        tmp = self.cross_attention(\n",
    "            x, cross, cross,\n",
    "        )\n",
    "        # [Add and Norm]\n",
    "        x = x + self.dropout(tmp)\n",
    "        y = x = self.norm1(x)\n",
    "\n",
    "        # [Feed Forward Add and Norm]\n",
    "        y = self.MLP1(y)\n",
    "        dec_output = self.norm2(x+y)\n",
    "        \n",
    "        dec_output = rearrange(dec_output, '(b ts_d) seg_dec_num d_model -> b ts_d seg_dec_num d_model', b = batch)\n",
    "        layer_predict = self.linear_pred(dec_output)\n",
    "        layer_predict = rearrange(layer_predict, 'b out_d seg_num seg_len -> b (out_d seg_num) seg_len')\n",
    "\n",
    "        return dec_output, layer_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    The decoder of Crossformer, making the final prediction by adding up predictions at each scale\n",
    "    '''\n",
    "    def __init__(self, seg_len, d_layers, d_model, n_heads, d_ff, dropout,\\\n",
    "                router=False, out_seg_num = 10, factor=10):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.router = router\n",
    "        self.decode_layers = nn.ModuleList()\n",
    "        for i in range(d_layers):\n",
    "            self.decode_layers.append(DecoderLayer(seg_len, d_model, n_heads, d_ff, dropout, \\\n",
    "                                        out_seg_num, factor))\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        final_predict = None\n",
    "        i = 0\n",
    "        ts_d = x.shape[1]\n",
    "        \n",
    "        for layer in self.decode_layers:\n",
    "            cross_enc = cross[i] # Select just one of the output layers from the encoder\n",
    "            x, layer_predict = layer(x,  cross_enc)\n",
    "            if final_predict is None:\n",
    "                final_predict = layer_predict\n",
    "            else:\n",
    "                final_predict = final_predict + layer_predict\n",
    "            i += 1\n",
    "        \n",
    "        final_predict = rearrange(final_predict, 'b (out_d seg_num) seg_len -> b (seg_num seg_len) out_d', out_d = ts_d)\n",
    "\n",
    "        return final_predict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### [1]: (1, data dimensions, seg_num, d_model) If you notice, these are the input dimensions that match the encoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from modules.encoder import Encoder\n",
    "from modules.decoder import Decoder\n",
    "from modules.layers import FullAttention, AttentionLayer, TwoStageAttentionLayer\n",
    "from modules.embed import DSW_embedding\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "class Crossformer(nn.Module):\n",
    "    def __init__(self, data_dim, in_len, out_len, seg_len, win_size = 2,\n",
    "                factor=10, d_model=512, d_ff = 1024, n_heads=8, e_layers=3, \n",
    "                dropout=0.0, baseline = False, device=torch.device('cuda:0')):\n",
    "        super(Crossformer, self).__init__()\n",
    "        self.data_dim = data_dim\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "        self.seg_len = seg_len\n",
    "        self.merge_win = win_size\n",
    "\n",
    "        self.baseline = baseline\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # The padding operation to handle invisible segment length\n",
    "        self.pad_in_len = ceil(1.0 * in_len / seg_len) * seg_len\n",
    "        self.pad_out_len = ceil(1.0 * out_len / seg_len) * seg_len\n",
    "        self.in_len_add = self.pad_in_len - self.in_len\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_value_embedding = DSW_embedding(seg_len, d_model)\n",
    "        self.enc_pos_embedding = nn.Parameter(torch.randn(1, data_dim, (self.pad_in_len // seg_len), d_model))\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(e_layers, win_size, d_model, n_heads, d_ff, block_depth = 1, \\\n",
    "                                    dropout = dropout,in_seg_num = (self.pad_in_len // seg_len), factor = factor)\n",
    "        \n",
    "        # Decoder\n",
    "        # [1]\n",
    "        self.dec_pos_embedding = nn.Parameter(torch.randn(1, data_dim, (self.pad_out_len // seg_len), d_model))\n",
    "        self.decoder = Decoder(seg_len, e_layers + 1, d_model, n_heads, d_ff, dropout, \\\n",
    "                                    out_seg_num = (self.pad_out_len // seg_len), factor = factor)\n",
    "        \n",
    "    def forward(self, x_seq):\n",
    "        if (self.baseline):\n",
    "            base = x_seq.mean(dim = 1, keepdim = True)\n",
    "        else:\n",
    "            base = 0\n",
    "        batch_size = x_seq.shape[0]\n",
    "        if (self.in_len_add != 0):\n",
    "            x_seq = torch.cat((x_seq[:, :1, :].expand(-1, self.in_len_add, -1), x_seq), dim = 1)\n",
    "\n",
    "        x_seq = self.enc_value_embedding(x_seq)\n",
    "        x_seq += self.enc_pos_embedding\n",
    "        x_seq = self.pre_norm(x_seq)\n",
    "        \n",
    "        enc_out = self.encoder(x_seq)\n",
    "\n",
    "        dec_in = repeat(self.dec_pos_embedding, 'b ts_d l d -> (repeat b) ts_d l d', repeat = batch_size)\n",
    "        predict_y = self.decoder(dec_in, enc_out)\n",
    "\n",
    "\n",
    "        return base + predict_y[:, :self.out_len, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
