use device: cuda:0
use device: cuda:0
data structure: [lines, timesteps, features]
train data size: [(122030, 120, 1)]
mytest data size: [(21356, 120, 1)]
Number of classes: 4
/root/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
  0%|                                                                                                         | 0/2 [00:00<?, ?it/s]/root/DNNM/mach5 baseline/run.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:199: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
tensor([[ 0.2754],
        [ 0.5189],
        [-0.0163],
        [ 0.0188],
        [-0.1956],
        [ 0.8902],
        [ 0.6430],
        [ 1.2229],
        [-0.0037],
        [ 1.3547],
        [-0.0043],
        [ 0.8280],
        [-0.2360],
        [ 0.7886],
        [-0.5397],
        [-0.1707],
        [-1.1595],
        [ 0.9648],
        [-0.2145],
        [-0.0794],
        [ 0.2993],
        [-0.1158],
        [-0.3014],
        [-0.0078],
        [ 0.1509],
        [-1.5795],
        [ 0.9567],
        [ 0.4560],
        [-0.0685],
        [ 0.0651],
        [-0.9456],
        [ 0.8336],
        [-0.1172],
        [-0.3376],
        [-0.4237],
        [ 0.1371],
        [-0.5511],
        [-0.6046],
        [-0.0065],
        [ 0.2518],
        [-0.1857],
        [-0.1494],
        [-0.0276],
        [-0.3215],
        [-1.4009],
        [-0.3624],
        [ 0.4251],
        [ 1.3565],
        [-1.2444],
        [ 0.4119],
        [-0.5715],
        [ 0.0094],
        [ 0.1210],
        [ 0.3787],
        [ 0.1695],
        [ 0.2303],
        [-0.1110],
        [ 1.4057],
        [ 0.0349],
        [-0.3260]], device='cuda:0')
tensor([[ 0.3073],
        [ 0.5673],
        [-0.0312],
        [ 0.0208],
        [-0.2131],
        [ 0.9802],
        [ 0.6979],
        [ 1.3159],
        [-0.0181],
        [ 1.4413],
        [-0.0181],
        [ 0.8703],
        [-0.2622],
        [ 0.8282],
        [-0.5804],
        [-0.1851],
        [-1.2135],
        [ 1.0130],
        [-0.2238],
        [-0.0824],
        [ 0.3165],
        [-0.1209],
        [-0.3139],
        [-0.0052],
        [ 0.1621],
        [-1.6650],
        [ 1.0150],
        [ 0.4844],
        [-0.0695],
        [ 0.0721],
        [-0.9833],
        [ 0.8848],
        [-0.1210],
        [-0.3528],
        [-0.4433],
        [ 0.1496],
        [-0.5725],
        [-0.6250],
        [ 0.0079],
        [ 0.2792],
        [-0.1861],
        [-0.1474],
        [-0.0181],
        [-0.3285],
        [-1.4671],
        [-0.3686],
        [ 0.4625],
        [ 1.4509],
        [-1.3013],
        [ 0.4489],
        [-0.5884],
        [ 0.0208],
        [ 0.1376],
        [ 0.4099],
        [ 0.1892],
        [ 0.2539],
        [-0.1088],
        [ 1.4970],
        [ 0.0464],
        [-0.3278]], device='cuda:0')
/root/DNNM/mach5 baseline/run.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
 50%|███████████████████████████████████████████████▌                                               | 1/2 [23:32<23:32, 1412.99s/it]
Traceback (most recent call last):
  File "/root/DNNM/mach5 baseline/run.py", line 270, in <module>
    train()
  File "/root/DNNM/mach5 baseline/run.py", line 190, in train
    loss.backward()
  File "/root/ml/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/root/ml/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt