use device: cuda:0
use device: cuda:0
data structure: [lines, timesteps, features]
train data size: [(122030, 120, 1)]
mytest data size: [(21356, 120, 1)]
Number of classes: 4
/root/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
  0%|                                                                                                           | 0/2 [00:00<?, ?it/s]/root/DNNM/mach5 baseline/run.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/root/DNNM/mach5 baseline/run.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:246: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
tensor([[ 0.2893],
        [ 0.5265],
        [-0.0101],
        [ 0.0357],
        [-0.1725],
        [ 0.8949],
        [ 0.6455],
        [ 1.2112],
        [ 0.0082],
        [ 1.3369],
        [ 0.0063],
        [ 0.8158],
        [-0.2159],
        [ 0.7775],
        [-0.5095],
        [-0.1504],
        [-1.1017],
        [ 0.9516],
        [-0.1885],
        [-0.0592],
        [ 0.3047],
        [-0.0931],
        [-0.2702],
        [ 0.0132],
        [ 0.1665],
        [-1.5170],
        [ 0.9532],
        [ 0.4619],
        [-0.0467],
        [ 0.0827],
        [-0.8940],
        [ 0.8312],
        [-0.0898],
        [-0.3017],
        [-0.3856],
        [ 0.1582],
        [-0.5102],
        [-0.5608],
        [ 0.0207],
        [ 0.2709],
        [-0.1522],
        [-0.1160],
        [ 0.0017],
        [-0.2851],
        [-1.3362],
        [-0.3254],
        [ 0.4375],
        [ 1.3496],
        [-1.1871],
        [ 0.4271],
        [-0.5256],
        [ 0.0342],
        [ 0.1405],
        [ 0.3901],
        [ 0.1882],
        [ 0.2490],
        [-0.0831],
        [ 1.3977],
        [ 0.0595],
        [-0.2890]], device='cuda:0')
tensor([[ 0.3073],
        [ 0.5673],
        [-0.0312],
        [ 0.0208],
        [-0.2131],
        [ 0.9802],
        [ 0.6979],
        [ 1.3159],
        [-0.0181],
        [ 1.4413],
        [-0.0181],
        [ 0.8703],
        [-0.2622],
        [ 0.8282],
        [-0.5804],
        [-0.1851],
        [-1.2135],
        [ 1.0130],
        [-0.2238],
        [-0.0824],
        [ 0.3165],
        [-0.1209],
        [-0.3139],
        [-0.0052],
        [ 0.1621],
        [-1.6650],
        [ 1.0150],
        [ 0.4844],
        [-0.0695],
        [ 0.0721],
        [-0.9833],
        [ 0.8848],
        [-0.1210],
        [-0.3528],
        [-0.4433],
        [ 0.1496],
        [-0.5725],
        [-0.6250],
        [ 0.0079],
        [ 0.2792],
        [-0.1861],
        [-0.1474],
        [-0.0181],
        [-0.3285],
        [-1.4671],
        [-0.3686],
        [ 0.4625],
        [ 1.4509],
        [-1.3013],
        [ 0.4489],
        [-0.5884],
        [ 0.0208],
        [ 0.1376],
        [ 0.4099],
        [ 0.1892],
        [ 0.2539],
        [-0.1088],
        [ 1.4970],
        [ 0.0464],
        [-0.3278]], device='cuda:0')
 50%|█████████████████████████████████████████████████                                                 | 1/2 [09:18<09:18, 558.32s/it]/root/DNNM/mach5 baseline/run.py:209: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
/root/DNNM/mach5 baseline/run.py:248: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
tensor([[ 0.3063],
        [ 0.5691],
        [-0.0305],
        [ 0.0226],
        [-0.2126],
        [ 0.9806],
        [ 0.6988],
        [ 1.3190],
        [-0.0159],
        [ 1.4479],
        [-0.0167],
        [ 0.8730],
        [-0.2629],
        [ 0.8288],
        [-0.5837],
        [-0.1861],
        [-1.2204],
        [ 1.0164],
        [-0.2246],
        [-0.0832],
        [ 0.3168],
        [-0.1211],
        [-0.3140],
        [-0.0041],
        [ 0.1634],
        [-1.6710],
        [ 1.0211],
        [ 0.4860],
        [-0.0690],
        [ 0.0730],
        [-0.9870],
        [ 0.8901],
        [-0.1192],
        [-0.3513],
        [-0.4420],
        [ 0.1532],
        [-0.5728],
        [-0.6265],
        [ 0.0084],
        [ 0.2806],
        [-0.1853],
        [-0.1460],
        [-0.0164],
        [-0.3275],
        [-1.4696],
        [-0.3678],
        [ 0.4636],
        [ 1.4541],
        [-1.3039],
        [ 0.4510],
        [-0.5890],
        [ 0.0214],
        [ 0.1383],
        [ 0.4109],
        [ 0.1904],
        [ 0.2555],
        [-0.1077],
        [ 1.5052],
        [ 0.0467],
        [-0.3293]], device='cuda:0')
tensor([[ 0.3073],
        [ 0.5673],
        [-0.0312],
        [ 0.0208],
        [-0.2131],
        [ 0.9802],
        [ 0.6979],
        [ 1.3159],
        [-0.0181],
        [ 1.4413],
        [-0.0181],
        [ 0.8703],
        [-0.2622],
        [ 0.8282],
        [-0.5804],
        [-0.1851],
        [-1.2135],
        [ 1.0130],
        [-0.2238],
        [-0.0824],
        [ 0.3165],
        [-0.1209],
        [-0.3139],
        [-0.0052],
        [ 0.1621],
        [-1.6650],
        [ 1.0150],
        [ 0.4844],
        [-0.0695],
        [ 0.0721],
        [-0.9833],
        [ 0.8848],
        [-0.1210],
        [-0.3528],
        [-0.4433],
        [ 0.1496],
        [-0.5725],
        [-0.6250],
        [ 0.0079],
        [ 0.2792],
        [-0.1861],
        [-0.1474],
        [-0.0181],
        [-0.3285],
        [-1.4671],
        [-0.3686],
        [ 0.4625],
        [ 1.4509],
        [-1.3013],
        [ 0.4489],
        [-0.5884],
        [ 0.0208],
        [ 0.1376],
        [ 0.4099],
        [ 0.1892],
        [ 0.2539],
        [-0.1088],
        [ 1.4970],
        [ 0.0464],
        [-0.3278]], device='cuda:0')
100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [16:51<00:00, 505.86s/it]