use device: cuda:0
use device: cuda:0
data structure: [lines, timesteps, features]
train data size: [(122030, 120, 1)]
mytest data size: [(21356, 120, 1)]
Number of classes: 4
/root/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
  0%|                                                                                                          | 0/20 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/root/DNNM/mach5 baseline/run.py", line 268, in <module>
    train()
  File "/root/DNNM/mach5 baseline/run.py", line 185, in train
    y_pre = net(x, y)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/root/DNNM/mach5 baseline/module/transformer.py", line 59, in forward
    out = self.decoder(tgt, memory, tgt_key_padding_mask = self.tgt_mask, memory_key_padding_mask = self.src_mask)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 712, in forward
    x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask, tgt_is_causal)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/root/ml/lib/python3.10/site-packages/torch/nn/functional.py", line 5312, in multi_head_attention_forward
    assert key_padding_mask.shape == (bsz, src_len), \
AssertionError: expecting key_padding_mask shape of (64, 60), but got torch.Size([60, 60])