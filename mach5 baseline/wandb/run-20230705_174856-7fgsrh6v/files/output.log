data structure: [lines, timesteps, features]
train data size: [(122030, 120, 1)]
mytest data size: [(21356, 120, 1)]
Number of classes: 4
/root/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
Traceback (most recent call last):
  File "/root/DNNM/mach5 baseline/infer.py", line 158, in <module>
    infer()
  File "/root/DNNM/mach5 baseline/infer.py", line 136, in infer
    predictions = run_encoder_decoder_inference(model=net,
  File "/root/DNNM/mach5 baseline/module/layers.py", line 213, in run_encoder_decoder_inference
    prediction = model(src, tgt, tgt_mask, src_mask)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/DNNM/mach5 baseline/module/infer_transformer.py", line 59, in forward
    out = self.decoder(tgt, memory, tgt_mask, src_mask)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 712, in forward
    x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask, tgt_is_causal)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1160, in forward
    return torch._native_multi_head_attention(
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mask in method wrapper_CUDA___native_multi_head_attention)