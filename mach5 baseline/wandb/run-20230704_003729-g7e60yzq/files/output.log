data structure: [lines, timesteps, features]
train data size: [(122053, 120, 1)]
mytest data size: [(21220, 120, 1)]
Number of classes: 4
  0%|                                                                                                                                                                                                                                                   | 0/6 [00:00<?, ?it/s]/root/ml/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 200, 1])) that is different to the input size (torch.Size([64, 200, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/root/DNNM/mach5 baseline/run.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/root/DNNM/mach5 baseline/run.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[-1].squeeze()
/root/DNNM/mach5 baseline/run.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[-1].squeeze()

 33%|█████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 2/6 [41:47<1:23:35, 1253.76s/it]
Traceback (most recent call last):
  File "/root/DNNM/mach5 baseline/run.py", line 262, in <module>
    train()
  File "/root/DNNM/mach5 baseline/run.py", line 196, in train
    torch.nn.utils.clip_grad_norm_(net.parameters(), .5)
KeyboardInterrupt