use device: cuda:0
use device: cuda:0
[[     0      1      2 ...    117    118    119]
 [     1      2      3 ...    118    119    120]
 [     2      3      4 ...    119    120    121]
 ...
 [143383 143384 143385 ... 143500 143501 143502]
 [143384 143385 143386 ... 143501 143502 143503]
 [143385 143386 143387 ... 143502 143503 143504]]
[[   119    120    121 ...    176    177    178]
 [   120    121    122 ...    177    178    179]
 [   121    122    123 ...    178    179    180]
 ...
 [143502 143503 143504 ... 143559 143560 143561]
 [143503 143504 143505 ... 143560 143561 143562]
 [143504 143505 143506 ... 143561 143562 143563]]
[[     0      1      2 ...    117    118    119]
 [     1      2      3 ...    118    119    120]
 [     2      3      4 ...    119    120    121]
 ...
 [143383 143384 143385 ... 143500 143501 143502]
 [143384 143385 143386 ... 143501 143502 143503]
 [143385 143386 143387 ... 143502 143503 143504]]
[[   119    120    121 ...    176    177    178]
 [   120    121    122 ...    177    178    179]
 [   121    122    123 ...    178    179    180]
 ...
 [143502 143503 143504 ... 143559 143560 143561]
 [143503 143504 143505 ... 143560 143561 143562]
 [143504 143505 143506 ... 143561 143562 143563]]
[[     0      1      2 ...    117    118    119]
 [     1      2      3 ...    118    119    120]
 [     2      3      4 ...    119    120    121]
 ...
 [143383 143384 143385 ... 143500 143501 143502]
 [143384 143385 143386 ... 143501 143502 143503]
 [143385 143386 143387 ... 143502 143503 143504]]
[[   119    120    121 ...    176    177    178]
 [   120    121    122 ...    177    178    179]
 [   121    122    123 ...    178    179    180]
 ...
 [143502 143503 143504 ... 143559 143560 143561]
 [143503 143504 143505 ... 143560 143561 143562]
 [143504 143505 143506 ... 143561 143562 143563]]
data structure: [lines, timesteps, features]
train data size: [(122030, 120, 1)]
mytest data size: [(21356, 120, 1)]
Number of classes: 4
/root/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
  0%|                                                                                                           | 0/2 [00:00<?, ?it/s]/root/DNNM/mach5 baseline/run.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:199: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
/root/ml/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/root/DNNM/mach5 baseline/run.py:234: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pre = torch.tensor(y_pre).cpu().detach().numpy()[0].squeeze()
/root/DNNM/mach5 baseline/run.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  act = torch.tensor(y).cpu().detach().numpy()[0].squeeze()
 50%|████████████████████████████████████████████████▌                                                | 1/2 [20:03<20:03, 1203.17s/it]
Traceback (most recent call last):
  File "/root/DNNM/mach5 baseline/run.py", line 268, in <module>
    train()
  File "/root/DNNM/mach5 baseline/run.py", line 190, in train
    loss.backward()
  File "/root/ml/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/root/ml/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt